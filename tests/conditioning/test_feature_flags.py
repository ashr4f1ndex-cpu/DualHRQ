"""
test_feature_flags.py
=====================

TDD tests for DRQ-104: Feature Flags and A/B Testing Infrastructure
These tests MUST be written first and will initially FAIL.
Implementation should make these tests pass.

CRITICAL: Safe rollout, performance isolation, A/B testing framework.
"""

import pytest
import numpy as np
import pandas as pd
from pathlib import Path
import sys
import time
import json
from typing import Dict, List, Any, Tuple, Optional, Callable
from datetime import datetime, timedelta
from unittest.mock import Mock, patch

# Add project root to path for imports
sys.path.append(str(Path(__file__).parent.parent.parent))

try:
    from src.models.conditioning.feature_flags import (
        FeatureFlagManager,
        ABTestManager,
        ExperimentConfig,
        FeatureGate,
        RolloutController,
        PerformanceIsolator
    )
except ImportError:
    # These will fail initially - that's expected in TDD
    pass


class TestFeatureFlagManager:
    """Tests for feature flag management system."""
    
    def test_basic_feature_flag_operations(self):
        """Test basic feature flag enable/disable operations."""
        # This will fail initially until FeatureFlagManager is implemented
        flag_manager = FeatureFlagManager()
        
        # Initially all flags should be disabled
        assert not flag_manager.is_enabled('new_conditioning'), \
            "New features should be disabled by default"
        
        # Enable a feature
        flag_manager.enable_feature('new_conditioning')
        assert flag_manager.is_enabled('new_conditioning'), \
            "Feature should be enabled after calling enable"
        
        # Disable a feature
        flag_manager.disable_feature('new_conditioning')
        assert not flag_manager.is_enabled('new_conditioning'), \
            "Feature should be disabled after calling disable"
        
        # Test batch operations
        features = ['pattern_library', 'rag_system', 'regime_features']
        flag_manager.enable_features(features)
        
        for feature in features:
            assert flag_manager.is_enabled(feature), \
                f"Feature {feature} should be enabled in batch operation"
    
    def test_percentage_rollout(self):
        """Test percentage-based feature rollouts."""
        flag_manager = FeatureFlagManager()
        
        # Set 30% rollout
        flag_manager.set_rollout_percentage('gradual_feature', 30)
        
        # Test with many user IDs to verify percentage
        enabled_count = 0
        total_tests = 1000
        
        for user_id in range(total_tests):
            if flag_manager.is_enabled_for_user('gradual_feature', f'user_{user_id}'):
                enabled_count += 1
        
        enabled_percentage = (enabled_count / total_tests) * 100
        
        # Should be close to target percentage (Â±5%)
        assert 25 <= enabled_percentage <= 35, \
            f"Rollout should be ~30%, got {enabled_percentage}%"
        
        # Same user should always get same result (deterministic)
        result1 = flag_manager.is_enabled_for_user('gradual_feature', 'consistent_user')
        result2 = flag_manager.is_enabled_for_user('gradual_feature', 'consistent_user')
        assert result1 == result2, "Same user should get consistent feature flag result"
    
    def test_user_targeting(self):
        """Test targeting specific users or user groups."""
        flag_manager = FeatureFlagManager()
        
        # Target specific users
        target_users = ['power_user_1', 'beta_tester_2', 'internal_user_3']
        flag_manager.set_user_targets('beta_feature', target_users)
        
        # Targeted users should have feature enabled
        for user in target_users:
            assert flag_manager.is_enabled_for_user('beta_feature', user), \
                f"Targeted user {user} should have feature enabled"
        
        # Non-targeted users should not
        assert not flag_manager.is_enabled_for_user('beta_feature', 'random_user'), \
            "Non-targeted user should not have feature enabled"
        
        # Test group targeting
        flag_manager.set_group_targets('internal_feature', ['internal', 'admin'])
        
        assert flag_manager.is_enabled_for_group('internal_feature', 'internal'), \
            "Targeted group should have feature enabled"
        assert not flag_manager.is_enabled_for_group('internal_feature', 'external'), \
            "Non-targeted group should not have feature enabled"
    
    def test_time_based_rollouts(self):
        """Test time-based feature rollouts and scheduling."""
        flag_manager = FeatureFlagManager()
        
        # Schedule feature to enable in the future
        future_time = datetime.now() + timedelta(hours=1)
        flag_manager.schedule_enable('scheduled_feature', future_time)
        
        # Should not be enabled yet
        assert not flag_manager.is_enabled('scheduled_feature'), \
            "Scheduled feature should not be enabled before scheduled time"
        
        # Mock time to test scheduling
        with patch('src.models.conditioning.feature_flags.datetime') as mock_datetime:
            mock_datetime.now.return_value = future_time + timedelta(minutes=5)
            assert flag_manager.is_enabled('scheduled_feature'), \
                "Scheduled feature should be enabled after scheduled time"
        
        # Test time-limited features
        start_time = datetime.now() - timedelta(hours=1)
        end_time = datetime.now() + timedelta(hours=1)
        
        flag_manager.set_time_window('limited_feature', start_time, end_time)
        
        assert flag_manager.is_enabled('limited_feature'), \
            "Feature should be enabled within time window"
    
    def test_conditional_flags(self):
        """Test conditional feature flags based on system state."""
        flag_manager = FeatureFlagManager()
        
        # Define condition function
        def high_performance_condition(context):
            return context.get('cpu_usage', 0) < 70 and context.get('memory_usage', 0) < 80
        
        flag_manager.set_condition('performance_feature', high_performance_condition)
        
        # Test with different contexts
        high_perf_context = {'cpu_usage': 50, 'memory_usage': 60}
        low_perf_context = {'cpu_usage': 90, 'memory_usage': 85}
        
        assert flag_manager.is_enabled_with_context('performance_feature', high_perf_context), \
            "Feature should be enabled under high performance conditions"
        
        assert not flag_manager.is_enabled_with_context('performance_feature', low_perf_context), \
            "Feature should be disabled under low performance conditions"
    
    def test_flag_dependencies(self):
        """Test feature flag dependencies and prerequisites."""
        flag_manager = FeatureFlagManager()
        
        # Set up dependency chain: advanced_feature requires basic_feature
        flag_manager.set_dependency('advanced_feature', 'basic_feature')
        
        # Enable advanced feature
        flag_manager.enable_feature('advanced_feature')
        
        # Should not be enabled without prerequisite
        assert not flag_manager.is_enabled('advanced_feature'), \
            "Advanced feature should not be enabled without prerequisite"
        
        # Enable prerequisite
        flag_manager.enable_feature('basic_feature')
        
        # Now both should be enabled
        assert flag_manager.is_enabled('basic_feature'), \
            "Basic feature should be enabled"
        assert flag_manager.is_enabled('advanced_feature'), \
            "Advanced feature should be enabled after prerequisite"


class TestABTestManager:
    """Tests for A/B testing framework."""
    
    def test_ab_test_setup(self):
        """Test setting up A/B tests with multiple variants."""
        # This will fail initially until ABTestManager is implemented
        ab_manager = ABTestManager()
        
        # Configure A/B test
        experiment_config = ExperimentConfig(
            name='conditioning_method_test',
            variants={
                'control': 0.5,      # 50% control (current method)
                'new_film': 0.25,    # 25% new FiLM conditioning
                'hybrid': 0.25       # 25% hybrid approach
            },
            start_date=datetime.now(),
            end_date=datetime.now() + timedelta(days=14),
            sample_size=10000
        )
        
        ab_manager.create_experiment(experiment_config)
        
        # Test assignment distribution
        assignments = {}
        for user_id in range(1000):
            variant = ab_manager.get_variant('conditioning_method_test', f'user_{user_id}')
            assignments[variant] = assignments.get(variant, 0) + 1
        
        # Check distribution is approximately correct
        total = sum(assignments.values())
        for variant, expected_pct in experiment_config.variants.items():
            actual_pct = assignments[variant] / total
            assert abs(actual_pct - expected_pct) < 0.05, \
                f"Variant {variant} should be ~{expected_pct:.0%}, got {actual_pct:.0%}"
    
    def test_ab_test_consistency(self):
        """Test that users get consistent A/B test assignments."""
        ab_manager = ABTestManager()
        
        config = ExperimentConfig(
            name='consistency_test',
            variants={'A': 0.5, 'B': 0.5},
            start_date=datetime.now(),
            end_date=datetime.now() + timedelta(days=7)
        )
        ab_manager.create_experiment(config)
        
        # User should get same variant across multiple calls
        user_id = 'consistent_user_123'
        
        variant1 = ab_manager.get_variant('consistency_test', user_id)
        variant2 = ab_manager.get_variant('consistency_test', user_id)
        variant3 = ab_manager.get_variant('consistency_test', user_id)
        
        assert variant1 == variant2 == variant3, \
            f"User should get consistent variant: {variant1}, {variant2}, {variant3}"
    
    def test_ab_test_metrics_collection(self):
        """Test collection of A/B test metrics and outcomes."""
        ab_manager = ABTestManager()
        
        config = ExperimentConfig(
            name='metrics_test',
            variants={'control': 0.5, 'treatment': 0.5},
            metrics=['sharpe_ratio', 'max_drawdown', 'total_return'],
            start_date=datetime.now(),
            end_date=datetime.now() + timedelta(days=30)
        )
        ab_manager.create_experiment(config)
        
        # Record metrics for different users
        test_data = [
            ('user_1', 'control', {'sharpe_ratio': 1.2, 'max_drawdown': -0.05, 'total_return': 0.08}),
            ('user_2', 'treatment', {'sharpe_ratio': 1.5, 'max_drawdown': -0.04, 'total_return': 0.12}),
            ('user_3', 'control', {'sharpe_ratio': 1.1, 'max_drawdown': -0.06, 'total_return': 0.07}),
        ]
        
        for user_id, expected_variant, metrics in test_data:
            # Verify user gets expected variant
            actual_variant = ab_manager.get_variant('metrics_test', user_id)
            
            # Record metrics
            ab_manager.record_metrics('metrics_test', user_id, metrics)
        
        # Analyze results
        results = ab_manager.analyze_experiment('metrics_test')
        
        assert 'control' in results, "Should have control group results"
        assert 'treatment' in results, "Should have treatment group results"
        
        for variant in ['control', 'treatment']:
            assert 'sharpe_ratio' in results[variant], f"Should track sharpe_ratio for {variant}"
            assert 'sample_size' in results[variant], f"Should track sample_size for {variant}"
    
    def test_statistical_significance_testing(self):
        """Test statistical significance calculations for A/B tests."""
        ab_manager = ABTestManager(min_sample_size=100)
        
        # Create experiment
        config = ExperimentConfig(
            name='significance_test',
            variants={'A': 0.5, 'B': 0.5},
            metrics=['conversion_rate'],
            significance_level=0.05
        )
        ab_manager.create_experiment(config)
        
        # Simulate data with clear difference
        # Group A: 10% conversion, Group B: 15% conversion
        for i in range(200):
            user_id = f'user_{i}'
            variant = ab_manager.get_variant('significance_test', user_id)
            
            if variant == 'A':
                conversion = 1 if np.random.random() < 0.10 else 0
            else:  # variant B
                conversion = 1 if np.random.random() < 0.15 else 0
            
            ab_manager.record_metrics('significance_test', user_id, {'conversion_rate': conversion})
        
        # Test significance
        significance_results = ab_manager.test_significance('significance_test', 'conversion_rate')
        
        assert 'p_value' in significance_results, "Should calculate p-value"
        assert 'confidence_interval' in significance_results, "Should calculate confidence interval"
        assert 'is_significant' in significance_results, "Should determine significance"
    
    def test_early_stopping(self):
        """Test early stopping of A/B tests based on results."""
        ab_manager = ABTestManager(early_stopping=True)
        
        config = ExperimentConfig(
            name='early_stop_test',
            variants={'control': 0.5, 'treatment': 0.5},
            metrics=['success_rate'],
            min_effect_size=0.05,
            early_stop_confidence=0.99
        )
        ab_manager.create_experiment(config)
        
        # Simulate strong treatment effect
        for i in range(500):
            user_id = f'user_{i}'
            variant = ab_manager.get_variant('early_stop_test', user_id)
            
            # Large effect: control 30%, treatment 50%
            success_rate = 0.3 if variant == 'control' else 0.5
            success = 1 if np.random.random() < success_rate else 0
            
            ab_manager.record_metrics('early_stop_test', user_id, {'success_rate': success})
            
            # Check if test should stop early
            if i > 100 and i % 20 == 0:  # Check every 20 samples after minimum
                should_stop = ab_manager.should_stop_early('early_stop_test')
                if should_stop:
                    break
        
        # Should have detected significant difference and stopped early
        experiment_status = ab_manager.get_experiment_status('early_stop_test')
        assert experiment_status['stopped_early'] or experiment_status['is_significant'], \
            "Should stop early with strong treatment effect"


class TestFeatureGate:
    """Tests for feature gate implementation in conditioning system."""
    
    def test_conditioning_method_gate(self):
        """Test feature gate for conditioning method selection."""
        # This will fail initially until FeatureGate is implemented
        gate = FeatureGate('conditioning_method')
        
        # Configure gate with multiple options
        gate.set_variants({
            'static_puzzle_id': 0.1,    # Legacy method (being phased out)
            'film_conditioning': 0.4,   # New FiLM method
            'hybrid_approach': 0.5      # Hybrid FiLM + RAG + Regime
        })
        
        # Test that gate returns valid conditioning method
        user_id = 'test_user_123'
        method = gate.get_variant(user_id)
        
        assert method in ['static_puzzle_id', 'film_conditioning', 'hybrid_approach'], \
            f"Gate should return valid conditioning method, got {method}"
        
        # Test method consistency
        method2 = gate.get_variant(user_id)
        assert method == method2, "Gate should return consistent method for same user"
    
    def test_pattern_library_gate(self):
        """Test feature gate for pattern library features."""
        gate = FeatureGate('pattern_library_features')
        
        # Gradual rollout of pattern library features
        gate.set_rollout_percentage(25)  # 25% of users get new features
        
        # Test with mock conditioning context
        def mock_conditioning_with_patterns(user_id):
            if gate.is_enabled(user_id):
                # Use pattern library
                return {'method': 'with_patterns', 'pattern_count': 150}
            else:
                # Use baseline conditioning
                return {'method': 'baseline', 'pattern_count': 0}
        
        # Test distribution
        with_patterns = 0
        total_tests = 400
        
        for i in range(total_tests):
            result = mock_conditioning_with_patterns(f'user_{i}')
            if result['method'] == 'with_patterns':
                with_patterns += 1
        
        pattern_percentage = (with_patterns / total_tests) * 100
        assert 20 <= pattern_percentage <= 30, \
            f"Pattern library should be enabled for ~25% of users, got {pattern_percentage}%"
    
    def test_performance_based_gating(self):
        """Test feature gates that adapt based on performance metrics."""
        gate = FeatureGate('adaptive_feature', performance_based=True)
        
        # Set performance thresholds
        gate.set_performance_thresholds({
            'min_sharpe_ratio': 1.0,
            'max_drawdown_threshold': -0.1,
            'min_success_rate': 0.6
        })
        
        # Test with good performance context
        good_performance = {
            'sharpe_ratio': 1.2,
            'max_drawdown': -0.05,
            'success_rate': 0.7
        }
        
        assert gate.is_enabled_with_performance('user_good', good_performance), \
            "Feature should be enabled with good performance"
        
        # Test with poor performance context
        poor_performance = {
            'sharpe_ratio': 0.5,
            'max_drawdown': -0.2,
            'success_rate': 0.4
        }
        
        assert not gate.is_enabled_with_performance('user_poor', poor_performance), \
            "Feature should be disabled with poor performance"
    
    def test_cascading_feature_gates(self):
        """Test cascading feature gates with dependencies."""
        # Set up gate hierarchy
        base_gate = FeatureGate('new_conditioning_system')
        pattern_gate = FeatureGate('pattern_library')
        rag_gate = FeatureGate('rag_system')
        
        # Set dependencies
        pattern_gate.set_prerequisite(base_gate)
        rag_gate.set_prerequisite(base_gate)
        
        # Enable base system for user
        base_gate.enable_for_user('test_user')
        pattern_gate.set_rollout_percentage(50)
        rag_gate.set_rollout_percentage(30)
        
        user_id = 'test_user'
        
        # Base should be enabled
        assert base_gate.is_enabled(user_id), "Base gate should be enabled"
        
        # Pattern and RAG may or may not be enabled (depends on rollout)
        pattern_enabled = pattern_gate.is_enabled(user_id)
        rag_enabled = rag_gate.is_enabled(user_id)
        
        # But if base is disabled, others should also be disabled
        base_gate.disable_for_user(user_id)
        
        assert not base_gate.is_enabled(user_id), "Base gate should be disabled"
        assert not pattern_gate.is_enabled(user_id), "Pattern gate should be disabled when base is disabled"
        assert not rag_gate.is_enabled(user_id), "RAG gate should be disabled when base is disabled"


class TestRolloutController:
    """Tests for safe rollout controller."""
    
    def test_gradual_rollout(self):
        """Test gradual rollout with automatic progression."""
        # This will fail initially until RolloutController is implemented
        controller = RolloutController(
            feature='new_hrm_integration',
            stages=[5, 10, 25, 50, 100],  # Rollout percentages
            stage_duration=timedelta(days=2),
            success_threshold=0.95
        )
        
        # Start rollout
        controller.start_rollout()
        
        # Should start at first stage
        current_stage = controller.get_current_stage()
        assert current_stage['percentage'] == 5, "Should start at 5% rollout"
        
        # Simulate successful metrics
        controller.record_stage_metrics({
            'success_rate': 0.97,
            'error_rate': 0.01,
            'performance_impact': 0.05
        })
        
        # Should be eligible for next stage
        assert controller.can_advance_stage(), "Should be able to advance with good metrics"
    
    def test_rollout_circuit_breaker(self):
        """Test circuit breaker functionality for failed rollouts."""
        controller = RolloutController(
            feature='risky_feature',
            stages=[10, 20, 40, 100],
            circuit_breaker_threshold=0.05  # 5% error rate threshold
        )
        
        controller.start_rollout()
        
        # Simulate high error rate
        controller.record_stage_metrics({
            'success_rate': 0.90,
            'error_rate': 0.10,  # Above threshold
            'performance_impact': 0.02
        })
        
        # Circuit breaker should trigger
        assert controller.is_circuit_breaker_triggered(), \
            "Circuit breaker should trigger with high error rate"
        
        # Rollout should be halted
        rollout_status = controller.get_rollout_status()
        assert rollout_status['halted'], "Rollout should be halted"
        assert rollout_status['reason'] == 'circuit_breaker', \
            "Should indicate circuit breaker reason"
    
    def test_automatic_rollback(self):
        """Test automatic rollback on severe issues."""
        controller = RolloutController(
            feature='automatic_rollback_test',
            stages=[20, 50, 100],
            auto_rollback=True,
            rollback_threshold={'error_rate': 0.15, 'performance_impact': 0.5}
        )
        
        controller.start_rollout()
        controller.advance_to_stage(1)  # Move to 50% rollout
        
        # Simulate severe issues
        controller.record_stage_metrics({
            'error_rate': 0.20,  # Above rollback threshold
            'performance_impact': 0.6,  # Above rollback threshold
            'success_rate': 0.4
        })
        
        # Should trigger automatic rollback
        assert controller.should_rollback(), "Should trigger rollback with severe issues"
        
        # Execute rollback
        controller.execute_rollback()
        
        rollout_status = controller.get_rollout_status()
        assert rollout_status['rolled_back'], "Should be rolled back"
        assert controller.get_current_rollout_percentage() == 0, \
            "Rollout percentage should be 0 after rollback"
    
    def test_rollout_monitoring(self):
        """Test real-time monitoring during rollout."""
        controller = RolloutController(
            feature='monitored_feature',
            monitoring_interval=timedelta(minutes=5)
        )
        
        controller.start_rollout()
        
        # Simulate metrics over time
        time_series_metrics = [
            {'timestamp': datetime.now(), 'success_rate': 0.95, 'latency': 100},
            {'timestamp': datetime.now() + timedelta(minutes=5), 'success_rate': 0.93, 'latency': 120},
            {'timestamp': datetime.now() + timedelta(minutes=10), 'success_rate': 0.91, 'latency': 150},
        ]
        
        for metrics in time_series_metrics:
            controller.record_time_series_metrics(metrics)
        
        # Should detect degrading performance
        trends = controller.analyze_trends()
        
        assert 'success_rate_trend' in trends, "Should analyze success rate trend"
        assert 'latency_trend' in trends, "Should analyze latency trend"
        
        # Should detect declining trend
        assert trends['success_rate_trend'] < 0, "Should detect declining success rate"


class TestPerformanceIsolation:
    """Tests for performance isolation during experiments."""
    
    def test_resource_isolation(self):
        """Test isolation of resources between control and treatment groups."""
        # This will fail initially until PerformanceIsolator is implemented
        isolator = PerformanceIsolator()
        
        # Configure resource limits
        isolator.set_resource_limits({
            'cpu_cores': {'control': 4, 'treatment': 2},
            'memory_gb': {'control': 8, 'treatment': 4},
            'gpu_memory_gb': {'control': 4, 'treatment': 2}
        })
        
        # Test resource allocation
        control_resources = isolator.get_allocated_resources('control')
        treatment_resources = isolator.get_allocated_resources('treatment')
        
        assert control_resources['cpu_cores'] == 4, "Control should get 4 CPU cores"
        assert treatment_resources['cpu_cores'] == 2, "Treatment should get 2 CPU cores"
        
        # Test isolation enforcement
        assert isolator.is_within_limits('control', {'cpu_usage': 3.5, 'memory_usage': 7}), \
            "Control group usage within limits should be allowed"
        
        assert not isolator.is_within_limits('treatment', {'cpu_usage': 3, 'memory_usage': 5}), \
            "Treatment group usage over limits should be blocked"
    
    def test_performance_impact_measurement(self):
        """Test measurement of performance impact between variants."""
        isolator = PerformanceIsolator(baseline_variant='control')
        
        # Record performance metrics
        isolator.record_performance('control', {
            'latency_ms': 45,
            'throughput_rps': 100,
            'cpu_utilization': 0.6,
            'memory_usage_mb': 2048
        })
        
        isolator.record_performance('treatment', {
            'latency_ms': 52,
            'throughput_rps': 95,
            'cpu_utilization': 0.7,
            'memory_usage_mb': 2500
        })
        
        # Calculate impact
        impact = isolator.calculate_performance_impact()
        
        assert 'latency_impact' in impact, "Should measure latency impact"
        assert 'throughput_impact' in impact, "Should measure throughput impact"
        
        # Treatment should show performance impact
        assert impact['latency_impact'] > 0, "Treatment should have higher latency"
        assert impact['throughput_impact'] < 0, "Treatment should have lower throughput"
    
    def test_interference_detection(self):
        """Test detection of interference between experiments."""
        isolator = PerformanceIsolator()
        
        # Set up multiple experiments
        isolator.register_experiment('exp1', ['variant_a', 'variant_b'])
        isolator.register_experiment('exp2', ['variant_x', 'variant_y'])
        
        # Simulate performance data
        isolator.record_performance('variant_a', {'metric': 100})
        isolator.record_performance('variant_b', {'metric': 90})
        isolator.record_performance('variant_x', {'metric': 85})  # Unexpectedly low
        isolator.record_performance('variant_y', {'metric': 95})
        
        # Check for interference
        interference_report = isolator.detect_interference()
        
        assert isinstance(interference_report, dict), "Should return interference report"
        # Interference detection logic will be implementation-specific
    
    def test_quality_gates(self):
        """Test quality gates for experiment validation."""
        isolator = PerformanceIsolator()
        
        # Set quality gates
        isolator.set_quality_gates({
            'max_latency_increase': 0.2,     # 20% max increase
            'min_throughput_ratio': 0.9,     # 90% min throughput
            'max_error_rate': 0.05,          # 5% max error rate
            'max_memory_increase': 0.3       # 30% max memory increase
        })
        
        # Test passing quality gates
        good_metrics = {
            'latency_increase': 0.15,
            'throughput_ratio': 0.95,
            'error_rate': 0.02,
            'memory_increase': 0.25
        }
        
        assert isolator.passes_quality_gates(good_metrics), \
            "Good metrics should pass quality gates"
        
        # Test failing quality gates
        bad_metrics = {
            'latency_increase': 0.35,  # Too high
            'throughput_ratio': 0.85,  # Too low
            'error_rate': 0.08,        # Too high
            'memory_increase': 0.4     # Too high
        }
        
        assert not isolator.passes_quality_gates(bad_metrics), \
            "Bad metrics should fail quality gates"
        
        # Get detailed gate results
        gate_results = isolator.evaluate_quality_gates(bad_metrics)
        assert 'passed' in gate_results, "Should indicate overall pass/fail"
        assert 'failures' in gate_results, "Should list specific failures"


class TestIntegrationScenarios:
    """Tests for integrated feature flag scenarios."""
    
    def test_end_to_end_conditioning_experiment(self):
        """Test complete conditioning method A/B test."""
        # Set up integrated test infrastructure
        flag_manager = FeatureFlagManager()
        ab_manager = ABTestManager()
        controller = RolloutController(
            feature='conditioning_experiment',
            stages=[10, 25, 50, 100]
        )
        
        # Configure A/B test
        experiment_config = ExperimentConfig(
            name='conditioning_method_comparison',
            variants={
                'static_puzzle_id': 0.33,
                'film_conditioning': 0.33,
                'hybrid_approach': 0.34
            },
            metrics=['sharpe_ratio', 'max_drawdown', 'prediction_accuracy'],
            start_date=datetime.now(),
            end_date=datetime.now() + timedelta(days=14)
        )
        
        ab_manager.create_experiment(experiment_config)
        controller.start_rollout()
        
        # Simulate user interactions
        for user_id in range(100):
            user_id_str = f'user_{user_id}'
            
            # Get user's variant
            variant = ab_manager.get_variant('conditioning_method_comparison', user_id_str)
            
            # Simulate conditioning system behavior based on variant
            if variant == 'static_puzzle_id':
                metrics = {
                    'sharpe_ratio': np.random.normal(1.0, 0.2),
                    'max_drawdown': np.random.normal(-0.08, 0.02),
                    'prediction_accuracy': np.random.normal(0.58, 0.1)
                }
            elif variant == 'film_conditioning':
                metrics = {
                    'sharpe_ratio': np.random.normal(1.2, 0.2),
                    'max_drawdown': np.random.normal(-0.06, 0.02),
                    'prediction_accuracy': np.random.normal(0.62, 0.1)
                }
            else:  # hybrid_approach
                metrics = {
                    'sharpe_ratio': np.random.normal(1.4, 0.2),
                    'max_drawdown': np.random.normal(-0.05, 0.02),
                    'prediction_accuracy': np.random.normal(0.68, 0.1)
                }
            
            # Record metrics
            ab_manager.record_metrics('conditioning_method_comparison', user_id_str, metrics)
        
        # Analyze experiment results
        results = ab_manager.analyze_experiment('conditioning_method_comparison')
        
        # Should have results for all variants
        for variant in experiment_config.variants.keys():
            assert variant in results, f"Should have results for {variant}"
            assert 'sharpe_ratio' in results[variant], f"Should track Sharpe ratio for {variant}"
        
        # Test rollout progression
        controller.record_stage_metrics({
            'success_rate': 0.95,
            'error_rate': 0.02,
            'performance_impact': 0.08
        })
        
        if controller.can_advance_stage():
            controller.advance_stage()
        
        rollout_status = controller.get_rollout_status()
        assert not rollout_status['halted'], "Rollout should not be halted with good metrics"


# This will run when pytest is called and should initially FAIL
# Implementation should make these tests pass
if __name__ == "__main__":
    pytest.main([__file__, "-v"])